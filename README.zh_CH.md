# YOLOv5.5 Detection with C#
---
**ç®€ä»‹:**
æœ¬é¡¹ç›®ä½¿ç”¨YOLOv5sæ¨¡å‹ï¼Œç”¨è‡ªå·±çš„æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼Œè¯†åˆ«LEDèŠ¯ç‰‡ï¼Œå¹¶ä¸”å°†ç½‘ç»œåœ¨C#ä¸­é€šè¿‡OpencvSharpçš„DNNæ¨¡å—è¿›è¡Œè°ƒç”¨æ¨ç†ï¼Œå°†ç¥ç»ç½‘ç»œåµŒå…¥åˆ°å®é™…çš„å·¥ç¨‹é¡¹ç›®ä¸­ã€‚  
**ä»»åŠ¡:** 
è¯†åˆ«LEDèŠ¯ç‰‡ï¼Œå¹¶ä¸”åœ¨C#ä¸­è¿›è¡Œæ¨ç†ã€‚  

---
**ç›®å½•ï¼š**
- [æ•°æ®é‡‡é›†](#1-æ•°æ®é‡‡é›†)
- [æ•°æ®é›†åˆ¶ä½œ](#2-æ•°æ®é›†åˆ¶ä½œ)
- [yoloæ¨¡å‹](#3-yoloæ¨¡å‹)
- [è®­ç»ƒåŠæ¨ç†](#4-è®­ç»ƒåŠæ¨ç†)
- [å¯¼å‡ºonnxæ¨¡å‹](#5-å¯¼å‡ºonnxæ¨¡å‹)
- [CSharpä¸­è°ƒç”¨onnxæ¨¡å‹](#6-csharpä¸­è°ƒç”¨onnxæ¨¡å‹)
- [[option]å¤šçº¿ç¨‹]()
---
## 1 æ•°æ®é‡‡é›† 
- ä½¿ç”¨å®é™…æ‘„åƒå¤´è¿›è¡Œé‡‡é›†çœŸå®çš„å›¾åƒï¼š  
  æ•°é‡ï¼š42 images  
<div align=center><img src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/datasets.png" width="70%" height="70%"></div>

## 2 æ•°æ®é›†åˆ¶ä½œ
- **æ•°æ®é›†åˆ’åˆ†**ï¼štraining:valid:test = 8:1:2
- **åˆ¶ä½œæ ‡ç­¾**ï¼š
  å·¥å…·ï¼š[labelImg](https://github.com/tzutalin/labelImg)
  æ ‡ç­¾æ–‡ä»¶: ```name.txt```ï¼Œæ¯ä¸€è¡Œä¸º ç±»åˆ« ä¸­å¿ƒx ä¸­å¿ƒy å®½ é«˜,åå­—nameå’Œå›¾ç‰‡ name.jpg å¯¹åº”ã€‚
    ```
    class x_center y_center width height
    ...
    ```
  1. åœ¨Pythonç¯å¢ƒä¸­å®‰è£…labelimg.
    ```shell
      pip install labelimg
    ```
  2. åˆ†å¥½æ•°æ®é›†åˆç»“æ„å¦‚ä¸‹ï¼š(è¿™é‡Œæ‰‹åŠ¨åˆ†æ•°æ®é›†)
    ```
    data/
        |-images/
            |-train/   
            |-valid/
        |-labels/
            |-train/
            |-valid/
    ```
  åˆ›å»ºç±»åˆ«æ–‡ä»¶(.txt),æ¯ä¸€è¡Œå†™ä¸€ç§ç±»åˆ«åç§°
  è¿™é‡Œåˆ›å»ºclasses.txtæ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š
    ```txt
    chip
    ```
  3. åœ¨ data/ æ–‡ä»¶å¤„æ‰“å¼€CMDã€ç»ˆç«¯ï¼Œå¦‚æœæ˜¯ä½¿ç”¨condaå®‰è£…çš„pythonç¯å¢ƒï¼Œæ³¨æ„æ¿€æ´»ç¯å¢ƒã€‚
    ```python
      >(conda activate [your-env-name])
      >labelimg data/images/train classes.txt
    ```
  å¯ä»¥çœ‹è§å¦‚ä¸‹å›¾ï¼Œé€‰æ‹©æ ‡ç­¾æ ¼å¼ä¸ºyoloï¼Œä¿å­˜è·¯å¾„ä¸º data/labels/trainï¼Œæ ‡ç­¾ï¼Œcost time!  
  <div align=center><img src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/labeling.png" width="50%" height="50%"></div>
  
## 3 yoloæ¨¡å‹
- **ä»€ä¹ˆæ˜¯YOLOvæ¨¡å‹ï¼Ÿ** see [Blog,Searchï¼Œæœ‰å¾ˆå¤šå¤§ä½¬å†™çš„ä¼˜ç§€çš„åšå®¢]()  
- **ç½‘ç»œæ•´ä½“ç»“æ„**  
   <div align=center>
  <img name="Struct" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png" width="75%" height="50%">
</div>  

  - è¿™æ¬¡ç›®æ ‡åªæœ‰LEDèŠ¯ç‰‡ï¼Œæ•…å¯¹äºæ¯ä¸€ä¸ªé¢„æµ‹æ¡†ï¼Œå…¶é¢„æµ‹çš„å‘é‡ä¸º x,y,h,w,p,cï¼Œpè¡¨ç¤ºç½®ä¿¡æ¦‚ç‡ï¼Œcè¡¨ç¤ºé¢„æµ‹ç±»åˆ«çš„æ¦‚ç‡ï¼Œæ•…å…¶é•¿åº¦ä¸º6ã€‚  

- **ä¸‹è½½yolov5.5å·¥ç¨‹æ–‡ä»¶**([ç‚¹æˆ‘ä¸‹è½½](https://github.com/ultralytics/yolov5/releases/tag/v5.0)),ä¸‹è½½æºæ–‡ä»¶ä»¥åŠé€‰æ‹©æƒé‡æ–‡ä»¶ï¼Œè¿™é‡Œé€‰æ‹©æƒé‡æ–‡ä»¶ä¸º**YOLOv5s**ã€‚
  <!-- Unzip Sources Zip file, In VSCode we can open this project and see:  -->
  è§£å‹æºæ–‡ä»¶ï¼Œåœ¨vscodeæ‰“å¼€å·¥ç¨‹æ–‡ä»¶ï¼š  
   <div align=center>
  <img name="YOLOv5 Og Project" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/originYOLO.png" width="50%" height="50%">
</div>

- **é…ç½®è‡ªå·±çš„å·¥ç¨‹æ–‡ä»¶**:
  - dataæ–‡ä»¶ï¼šä¿å­˜æ•°æ®æ–‡ä»¶é…ç½® .yaml æ ¼å¼ï¼Œæˆ‘ä»¬éœ€è¦æ ¹æ®è‡ªå·±çš„æ•°æ®é›†ã€è®­ç»ƒç›®æ ‡ï¼Œå»ºç«‹è‡ªå·±çš„dataé…ç½®æ–‡ä»¶ï¼Œè¿™ä¸ªé¡¹ç›®é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š
    ```yaml
    # train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]
    # the project will auto go to ../data/labels/train/ to get labels.
    train: ../data/images/train/  # train images  
    val: ../data/images/valid/  # valid images  

    # number of classes
    nc: 1

    # class names
    names: [ 'chip' ]  
    ```
  
  - modelé…ç½®æ–‡ä»¶ï¼šæ ¹æ®é€‰æ‹©æ¨¡å‹ï¼Œåœ¨/model/æ–‡ä»¶å¤¹ä¸­ï¼Œé€‰æ‹©ç›¸åº”çš„é…ç½®æ–‡ä»¶ï¼Œè¿™é‡Œé€‰æ‹©```yolov5s.yaml```æ–‡ä»¶ï¼Œå¤åˆ¶ä¿®æ”¹åï¼Œåªä¿®æ”¹ parametersä¸­çš„ncå‚æ•°ä¸º1ï¼Œå³ç±»åˆ«æ•°é‡ï¼Œä¿®æ”¹åå¦‚ä¸‹ï¼š
    ```yaml
      # parameters
      nc: 1  # number of classes
      depth_multiple: 0.33  # model depth multiple
      width_multiple: 0.50  # layer channel multiple
      # anchors æ¨¡å‹é”šæ¡†
      anchors:
        - [10,13, 16,30, 33,23]  # P3/8
        - [30,61, 62,45, 59,119]  # P4/16
        - [116,90, 156,198, 373,326]  # P5/32
      # æ¡†æ¶ä¸å˜
      # YOLOv5 backbone
      backbone:
      # YOLOv5 head
    ```
  
  - ä¸‹è½½é¢„è®­ç»ƒæƒé‡([ç‚¹æˆ‘ä¸‹è½½](https://github.com/ultralytics/yolov5/releases/tag/v5.0)),æ·»åŠ åˆ°å·¥ç¨‹æ–‡ä»¶ä¸­  
  - è®­ç»ƒé…ç½®æ–‡ä»¶ï¼šå‚è€ƒ `data/hyp.scratch.yaml` æ–‡ä»¶,å¯è®¾ç½®å¦‚è®­ç»ƒæ—¶çš„å›¾åƒå¢å¼ºã€ioué˜ˆå€¼ã€Mixupæ¦‚ç‡ç­‰ã€‚  
- **pythonç¯å¢ƒ**: `python>=3.8`
    ```shell
      > pip install -r requirements.txt
    ```
## 4 è®­ç»ƒåŠæ¨ç†
- **ä¿®æ”¹ ```train.py``` æ–‡ä»¶**
  ä¸»è¦ä¿®æ”¹å‚æ•°ï¼š
  ```
    --weights: ä¸‹è½½çš„é¢„è®­ç»ƒæƒé‡åœ°å€
    --cfg: modelé…ç½®æ–‡ä»¶åœ°å€ï¼Œæ­¥éª¤3åˆ›å»ºçš„é…ç½®æ–‡ä»¶ï¼Œå¦‚yolov5s.yaml
    --data: dataé…ç½®æ–‡ä»¶åœ°å€
    --batch-sizeï¼šæ ¹æ®å®é™…æƒ…å†µ
    --img sizeï¼š32çš„å€æ•°é»˜è®¤640x640
    --epochs: æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©
    --device: GPUæ›´å¿«
    --workersï¼šçº¿ç¨‹
  ```
  ä¿®æ”¹åå¦‚ä¸‹,(åªæ˜¾ç¤ºæ”¹å˜éƒ¨åˆ†)ï¼š
  ```python
    if __name__ == '__main__':
      parser = argparse.ArgumentParser()
      parser.add_argument('--weights', type=str, default='yolov5-5.0/weights/yolov5s.pt', help='initial weights path')
      parser.add_argument('--cfg', type=str, default='yolov5-5.0/models/myyolov5s.yaml', help='model.yaml path')
      parser.add_argument('--data', type=str, default='yolov5-5.0/data/chips.yaml', help='data.yaml path')
      parser.add_argument('--epochs', type=int, default=500)
      parser.add_argument('--batch-size', type=int, default=4, help='total batch size for all GPUs')
      parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='[train, test] image sizes')
      parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
      parser.add_argument('--workers', type=int, default=2, help='maximum number of dataloader workers')
  ```
- **Run!**ï¼šè¿è¡Œ`train.py`ç¨‹åº And **Wait... Get yourself some Tea ğŸµand hear win~win~win!**
   è®­ç»ƒç»“æŸåï¼šä¼šåœ¨å½“å‰è·¯å¾„ä¸‹ï¼Œç”Ÿæˆ```run/train/```æ–‡ä»¶å¤¹è¾“å‡ºè®­ç»ƒç»“æœã€æƒé‡:
   <div align=center>
    <img name="TrainOutputFile" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/train_Output.png" width="70%" height="70%"></div>

   æƒé‡ä¿å­˜åœ¨```./run/train/weight/```ä¸­ï¼Œä½ å°†å¯ä»¥çœ‹åˆ°`best.pt`å’Œ`last.pt`ä¸¤ä¸ªptæ¨¡å‹æ–‡ä»¶ã€‚
- **Detect!**: è·Ÿè®­ç»ƒæ—¶ç±»ä¼¼ï¼Œæ‰“å¼€`detect.py`æ–‡ä»¶ï¼ŒåŒæ ·éœ€è¦ä¿®æ”¹ä¸€äº›å‚æ•°ï¼Œç„¶åç›´æ¥runå°±è¡Œï¼š  
    ```
      --weight   ;è®­ç»ƒåçš„`.pt`æƒé‡æ–‡ä»¶
      --source   ;æ¨ç†å›¾ç‰‡æˆ–è€…è§†é¢‘
      --img-size ;æ¨¡å‹è¾“å…¥å›¾ç‰‡å¤§å°
      [optional] --conf-thres  ;ç½®ä¿¡åº¦
      [optional] --iou-thres   ;ioué˜ˆå€¼
      [optional] --device
    ```  
    ä¿®æ”¹åå¦‚ä¸‹ï¼š  
    ```python
    if __name__ == '__main__':
      parser = argparse.ArgumentParser()
      parser.add_argument('--weights', nargs='+', type=str, default='runs\\train\\exp5\\weights\\best.pt', help='model.pt path(s)')
      parser.add_argument('--source', type=str, default='yolov5-5.0\\myDatasets2\\chip\\images\\test\\8.jpg', help='source')  # file/folder, 0 for webcam
      parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')
      parser.add_argument('--conf-thres', type=float, default=0.8, help='object confidence threshold')
      parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')
      parser.add_argument('--device', default='0', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
    ```  
    å¾—åˆ°ç»“æœå¦‚ä¸‹ï¼š  
    <div align=center>
      <img name="DetectImage" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/detectImg.jpg" width="60%" height="60%"></div>  
    å¯ä»¥çœ‹åˆ°ï¼Œå°½ç®¡ä½¿ç”¨çš„æ•°æ®é›†åˆå°ï¼Œä½†ä»è¡¨ç°ä¸é”™ï¼Œåªä¸€å¼ å›¾ç‰‡ï¼Œåœ¨æˆ‘çš„ç”µè„‘(Intel i7-8565U)ä¸Šç”¨CPUä¸Šæ¨ç†0.08s,ç”»å›¾,0.11så·¦å³ã€‚
## 5 å¯¼å‡ºonnxæ¨¡å‹  
- **YOLOv5æ¨¡å‹æ¦‚è¿°**: YOLOv5.5åœ¨ä¸‰ç§ä¸åŒçš„å¤§å°çš„ç‰¹å¾å›¾è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œä»¥æ¯ä¸€ä¸ªç‰¹å¾å›¾çš„pixieä¸ºä¸­å¿ƒï¼Œç”¨ä¸‰ç§ä¸åŒé•¿å®½æ¯”çš„anchorè¿›è¡Œé¢„æµ‹ï¼ŒåŸæœ¬çš„ç½‘ç»œè¾“å‡ºå¯¹å¯¹åº”æ¯ä¸€ä¸ªç‰¹å¾å›¾ä¸Šçš„æ¯ä¸€ä¸ªåƒç´ ï¼Œè¾“å‡ºä¸€ä¸ªé¢„æµ‹å‘é‡{x,y,h,w,confidence,[classify results]}ï¼Œé•¿åº¦ä¸º`4+1+num of classes`ã€‚`detect`ç”¨confidenceä»¥åŠnmsç®—æ³•å¯¹å„ç§é¢„æµ‹é”šæ¡†è¿›è¡Œç­›é€‰ã€‚
  - å¯¹äºé¢„æµ‹ç»“æœçš„ç›®æ ‡æ¡†å›å½’ï¼šæ ¹æ®å·¥ç¨‹æ–‡ä»¶ä¸­`model/yolo.py/`æ–‡ä»¶ä¸­çš„`Detect.forward()`éƒ¨åˆ†ï¼Œå…¶ä»£ç å¦‚ä¸‹:  
    ```python []
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()

            if not self.training:  # inference
                if self.grid[i].shape[2:4] != x[i].shape[2:4]:
                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)

                y = x[i].sigmoid()
                y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy
                y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh
                z.append(y.view(bs, -1, self.no))
        return x if self.training else (torch.cat(z, 1), x)
    ```  

    ![1](http://latex.codecogs.com/svg.latex?\begin{cases}x=(2Sigmoid(x)-0.5+c)*stride\\\\y=(2Sigmoid(y)-0.5+r)*stride\\\\h=(2Sigmoid(h))^2*anchor_h\\\\w=(2Sigmoid(w))^2*anchor_w\\\\\end{cases})  
    - cè¡¨ç¤ºå½“å‰ä½ç½®åœ¨ç‰¹å¾å›¾ä¸Šçš„å®½åº¦åæ ‡(å³åƒç´ åæ ‡)ï¼Œè¡¨ç¤ºåç§»é‡
    - rè¡¨ç¤ºå½“å‰ä½ç½®åœ¨ç‰¹å¾å›¾ä¸Šçš„é«˜åº¦åæ ‡(å³åƒç´ åæ ‡)ï¼Œè¡¨ç¤ºåç§»é‡
    - strideï¼Œå‹ç¼©åˆ°ä¸åŒå¤§å°çš„ç‰¹å¾å›¾æ—¶çš„ä½¿ç”¨ä¸åŒstride
    - anchor:anchor_hã€anchor_w,ä¸åŒanchorå¤§å°å‚æ•°
    - tipsï¼šå‚è€ƒæ¨¡å‹é…ç½®æ–‡ä»¶ï¼Œanchor={h1,h2,h3,w1,w2,w3},stride={8,16,32}

- **æŠŠç»“æœè¾“å‡ºçš„ä¸‰ä¸ªç‰¹å¾å›¾concatæˆäºŒç»´çš„å¼ é‡**  
  - æ­£å¸¸yolov5è¾“å‡ºç»“æ„:   
      å‡è®¾ç½‘ç»œè¾“å…¥å¤§å°ä¸º[h,w],ç‰¹å¾å›¾æ­¥é•¿stride={s1,s2,s3}ï¼Œanchoræ•°é‡ä¸º3ï¼Œè¾“å‡ºç»“æœå¦‚ä¸‹ï¼š    
      stride = s1: `1,(5+num of classes),3(h/s1),3(w/s1)`  
      stride = s2: `1,(5+num of classes),3(h/s2),3(w/s2)`  
      stride = s3: `1,(5+num of classes),3(h/s3),3(w/s3)`  
  - ä¿®æ”¹ï¼šflattenæˆäºŒç»´ï¼Œå¯¹strideã€anchoråœ¨(5+num of classes)ç»´åº¦ä¸Šæ‹¼æ¥ï¼Œå¯¹äºyolov5.5sæœ€ç»ˆè¾“å‡ºæ ¼å¼ä¸ºï¼š25200x(5+num of classes)  
  - (å½“ç„¶ä¹Ÿå¯ä»¥ä¸ä¿®æ”¹ï¼Œåªæ˜¯åç»­å¯¹ç»“æœå¤„ç†éœ€è¿›è¡Œä¸€å®šçš„æ”¹å˜)    
  å¯¹`model/yolo.py`ä¸­çš„**Detectç±»çš„forward()** ä¿®æ”¹å¦‚ä¸‹ï¼š
  ```python
        def forward(self, x):
        #å¯¼å‡ºonnxåˆ°opencv
        x = x.copy()  # for profiling
        z = []  # inference output
        self.training |= self.export
        for i in range(self.nl):
            x[i] = self.m[i](x[i])  # conv
            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)
            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()
            x[i] = x[i].view(bs * self.na * ny * nx, self.no).contiguous() # reshapeæˆäºŒç»´
        return torch.cat(x) #batchè¿›è¡ŒConcat
  ```    
- **Output ONNX æ¨¡å‹ï¼**  
  - **æ–¹æ³•**: `model/export.py`   
  - åŒæ ·ä¿®æ”¹ä¸€äº›é…ç½®:  
  ```python
    if __name__ == '__main__':
      parser = argparse.ArgumentParser()
      parser.add_argument('--weights', type=str, default='runs\\train\\exp2\\weights\\best.pt', help='weights path')  # from yolov5/models/
      parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='image size')  # height, width
      parser.add_argument('--batch-size', type=int, default=1, help='batch size')
      parser.add_argument('--dynamic', action='store_true', help='dynamic ONNX axes')
      parser.add_argument('--grid', action='store_true', help='export Detect() layer grid')
      parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')
  ```   
  - Run! ç„¶åonnxæ–‡ä»¶ä¼šä¿å­˜åˆ°ä¸`.pt` æ–‡ä»¶ç›¸åŒæ–‡ä»¶ç›®å½•ä¸‹ã€‚   
  - **é€šè¿‡netronæŸ¥çœ‹onnxæ¨¡å‹**ï¼ŒåŸæ¥è¾“å‡ºï¼šå±•å¼€åè¾“å‡ºï¼š  
   <div class="content_img" align=center>
      <img name="OriginONNXOutput" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/OriginONNXOutput.png" width="300" height="300">     <img name="FlattenOutput" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/FlattenOutput.png" width="300" height="300"><div>Left:Origin & Right:Flattened</div>  
  
## 6 CSharpä¸­è°ƒç”¨onnxæ¨¡å‹
  **tool**: `opencvsharp.dnn`[Here](https://github.com/shimat/opencvsharp/releases/tag/4.5.3.20211228).   
            `VS2022`[VS2022](https://visualstudio.microsoft.com/).    
- éœ€è¦çš„åŠ¨æ€é“¾æ¥åº“(DLL):
   `OpencvSharp` åœ¨vsçš„C#ä¸­æ·»åŠ å¼•ç”¨ï¼Œæ ¹æ®.NETç‰ˆæœ¬é€‰æ‹©opencvåŒ…ä¸­çš„ `opencvsharp.dll`ä»¥åŠ`opecvsharp.Extention.dll`ï¼Œå¹¶ä¸”æŠŠçš„`OpenCvSharp-4.5.3-20211228/NativeLib/win/x64/OpenCvSharpExtern.dll"`å¤åˆ¶åˆ°vså·¥ç¨‹æ–‡ä»¶çš„debugã€releaseæ–‡ä»¶ä¸­ã€‚  
  - åœ¨C#æ–‡ä»¶ä¸­å¼•ç”¨:  
  ```c#
    using OpenCvSharp;
    using OpenCvSharp.Dnn;
  ```
- **ç°åœ¨ï¼Œåˆ›å»ºç›´æ¥çš„yoloç±»å§!**:
  - è¯»å–å°†ONNXæ–‡ä»¶æ·»åŠ åˆ°åœ¨å·¥ç¨‹æ–‡ä»¶ä¸­ä¸/bin/æ–‡ä»¶åŒä¸€ä¸ªçˆ¶ç›®å½•ä¸‹ï¼Œåˆ›å»º/Resource/æ–‡ä»¶ï¼Œä¿å­˜onnxç½‘ç»œæ–‡ä»¶ï¼š
  ```c#
        /// <summary>
        /// è¯»å–onnxæ¨¡å‹
        /// </summary>
        /// <param name="path">ç½‘ç»œæ–‡ä»¶è·¯å¾„</param>
        /// <returns></returns>
        public bool readModel(string path)
        {
            try
            {
                net = Net.ReadNetFromONNX(path);  //è¯»å–ç½‘ç»œ
                isReadSuccess = true;
            }
            catch (Exception)
            {
                net = null;
                isReadSuccess = false;
                return false;
            }
            net.SetPreferableBackend(Backend.DEFAULT);  //CPUè¿è¡Œ
            net.SetPreferableTarget(Target.CPU);
            return true;
        }
  ```  
  - è®¾ç½®å„ç§å±æ€§:
  ```c#
      private  Net net;  //ç½‘ç»œ
      public bool isReadSuccess = false;
      /// <summary>
      /// yolov5ç½‘ç»œå‚æ•°
      /// </summary>
      // 1. anchors é”šæ¡†ï¼Œåé¢å›å½’éœ€è¦ç”¨åˆ°
      //      ç”±äºé‡‡æ ·çš„å›¾ç‰‡é•¿å®½æ¯”ä¸å¤§ï¼Œæ•…ç›´æ¥ç”¨åŸå§‹é”šæ¡†å¤§å°
      //      å¯¹äº3ç§ä¸åŒç‰¹å¾å›¾çš„é”šæ¡†ï¼Œå¯¹äºæ¯ä¸€ä¸ªç‰¹å¾å›¾æœ‰3ç§é”šæ¡†
      //      å°ç‰¹å¾å›¾ï¼Œç”¨å¤§çš„é”šæ¡†å¯ä»¥æœç´¢æ›´å¤§çš„ç‰©ä½“ï¼Œè€Œå¤§ç‰¹å¾å›¾ç”¨å°çš„é”šæ¡†å¯ä»¥æœç´¢æ›´å°çš„ç‰©ä½“
      float [,] netAnchors = new float[3,6]{
      { 10.0F, 13.0F, 16.0F, 30.0F, 33.0F, 23.0F }, // å¤§ç‰¹å¾å›¾
      { 30.0F, 61.0F, 62.0F, 45.0F, 59.0F, 119.0F }, //ä¸­ç‰¹å¾å›¾
      { 116.0F, 90.0F, 156.0F, 198.0F, 373.0F, 326.0F}}; //å°ç‰¹å¾å›¾
      // 2. strideï¼Œé”šæ¡†çš„æ­¥é•¿
      //      å³å¯¹åº”ä¸‰ç§ç‰¹å¾å›¾åœ¨é™ç»´æ—¶ç”¨çš„æ­¥é•¿ï¼Œæ ¹æ®è¿™ä¸ªå¯ä»¥å¾—åˆ°ç‰¹å¾å›¾çš„boxä¸ªæ•°
      float[] netStride = new float[3] { 8.0F, 16.0F, 32.0F};
      // 3. è¾“å…¥å›¾åƒå¤§å° 32å€æ•°
      //    è¿™é‡Œä¸º640x640
      float netHeight = 640;
      float netWidth = 640;
      // 4. å„ç§åˆå§‹ç½®ä¿¡æ¦‚ç‡(é˜ˆå€¼)
      //    å¯æ”¹
      float nmsThreshold = 0.45f;  //nmsé˜ˆå€¼
      float boxThreshold = 0.5f;  //ç½®ä¿¡åº¦é˜ˆå€¼
      float classThreshold = 0.45f; //ç±»åˆ«é˜ˆå€¼
      List<string> classname = new List<string>{ "chip" };
  ```   
- **æ¨ç†Detect!** ï¼Œåœ¨C#ä¸­æˆ‘ä»¬åªéœ€åº”ç”¨ç½‘ç»œè¿›è¡Œæ¨ç†å³å¯
  - ç½‘ç»œè§„å®šçš„å›¾åƒå¤§å° `[640,640]`ï¼Œä½†æˆ‘ä»¬å®é™…çš„å›¾ç‰‡å¹¶ä¸æ˜¯ç›¸åŒå¤§å°ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¹å…¶è¾“å…¥è¿›è¡Œresizeï¼Œä¹Ÿéœ€è¦æŠŠè¾“å‡ºæ¡†è¿›è¡Œresizeï¼›  
    ```c#
      // 1) æ ¼å¼åŒ–è¾“å…¥å›¾åƒï¼Œå˜æˆ 1x3x640x640çš„tensorï¼Œå¹¶ä¸”å½’ä¸€åŒ–ï¼Œ
      // scaleFactorå½’ä¸€åŒ–
      // swapRB:BGR -> RGB
      Mat blob = CvDnn.BlobFromImage(ScrImg, scaleFactor:1.0 /255, size:new OpenCvSharp.Size(netWidth,netHeight), mean:new Scalar(0,0,0), swapRB:false,crop:fals);
      // 2) é«˜ã€å®½çš„ç¼©æ”¾å¤§å°å€ç‡,ç”¨äºæœ€åè¾“å‡ºé”šæ¡†è¿›è¡Œresizeï¼Œå¤åŸåˆ°åŸå§‹å›¾åƒå¤§å°
      double ratio_h = ScrImg.Rows /netHeight;  // è¾“å…¥å›¾åƒçš„é«˜: 640
      double ratio_w = ScrImg.Cols / netWidth;  // è¾“å…¥å›¾åƒçš„å®½: 640
    ```  
  - Inference å‘å‰ä¼ æ’­ï¼
    ```c#
      // 3) è¾“å…¥ç½‘ç»œ
      //DateTime t1 = DateTime.Now;
      net.SetInput(blob);
      // 4) å‘å‰ä¼ æ’­ 
      Mat netOutput = net.Forward();  //25200x6
    ```  
  - å–å‡ºè¾“å‡ºç»“æœçš„æ¯ä¸€è¡Œï¼Œç­›é€‰ç½®ä¿¡ç‡é«˜çš„ï¼Œé”šæ¡†å›å½’ï¼Œæ·»åŠ ä¸ºå¤‡é€‰æ¡†
    ```c#
      int r = 0;  // indexï¼Œç¬¬ä¸€è¡Œ
      List<int> classIds = new List<int>(); //ä¿å­˜å¥½çš„é”šæ¡†çš„åˆ†ç±»ç»“æœ
      List<float> confidences = new List<float>(); //ä¿å­˜å¥½çš„é”šæ¡†çš„ç½®ä¿¡
      List<Rect> bouduary = new List<Rect>();     //ä¿å­˜å¥½çš„é”šæ¡†x,y,h,w
      // 5) éå†æ‰€æœ‰é¢„æµ‹é”šæ¡†å‚æ•°
      //    æ³¨ï¼šç½‘ç»œçš„è¾“å‡ºæ ¼å¼å·²ç»å°†3ä¸ªç‰¹å¾å›¾å¯¹åº”çš„æ‰€æœ‰boxçš„é¢„æµ‹çš„3ç§ä¸åŒé”šæ¡†ï¼Œflattenæˆ äºŒç»´çŸ©é˜µ
      //        ä»å¤§ç‰¹å¾å›¾å¼€å§‹çš„å±•å¼€ï¼›
      //å…ˆæŒ‰ç‰¹å¾å›¾å¤§å°è¿›è¡Œforéå†
      for (int s = 0; s < 3; s++)
      {
          // è·å–boxä¸ªæ•°
          int grid_x = (int)(netWidth / netStride[s]);
          int grid_y = (int)(netHeight / netStride[s]);
          //MessageBox.Show(grid_x.ToString()); //debugç”¨
          //MessageBox.Show(grid_y.ToString());
          // å¯¹æ¯ä¸ªanchorè¿›è¡Œéå†
          for (int anchor = 0; anchor < 3; anchor++)
          {
              // å¾—åˆ°anchoré«˜ã€å®½å¤§å°
              double anchor_w = netAnchors[s, anchor * 2];
              double anchor_h = netAnchors[s, anchor * 2 + 1];
              // å¯¹æ¯ä¸€ä¸ªboxçš„é¢„æµ‹ç»“æœè¿›è¡Œéå†
              for (int j = 0; j < grid_y; j++)
              {
                  for (int i = 0; i < grid_x; i++)
                  {
                      // åœ¨è¾“å‡ºç»“æœä¸­è·å–å¯¹åº”çš„boxçš„é¢„æµ‹ç»“æœ 1x6çš„å‘é‡
                      Mat pMat = netOutput.Row(r);
                      float[] pdata;
                      // å˜æˆarrayï¼Œæ›´å¥½ç”¨
                      pMat.GetArray(out pdata);
                      // ç¬¬äº”ä¸ªå³ä¸ºç½®ä¿¡æ¦‚ç‡
                      double box_score = Sigmoid(pdata[4]);
                      if (box_score > boxThreshold)
                      {
                          //++num;
                          // pdataç¬¬å…­ä¸ªå¼€å§‹ä¸ºç±»åˆ«é¢„æµ‹çš„æ¦‚ç‡ï¼Œå³ç¬¬äº”ä¸ªä¹‹åçš„å‘é‡ä¸ºone-hotè¾“å‡ºç»“æœ
                          // classifyOutputä¿å­˜one-hotè¾“å‡ºç»“æœ
                          float[] classifyOutput = pdata.Skip(5).ToArray();   
                          Mat score = new Mat(1,classname.Count(),MatType.CV_32FC1, classifyOutput); //è½¬æˆ1,ç±»åˆ«æ•°çš„äºŒç»´Mat
                          OpenCvSharp.Point classIdpoint = new OpenCvSharp.Point();
                          double max_class_score;
                          // è·å–æœ€å¤§å€¼åŠå…¶å¯¹åº”çš„indexå³åˆ†ç±»çš„ç±»åˆ«id
                          Cv2.MinMaxLoc(score, out double min_class_score, out max_class_score, out OpenCvSharp.Point minclassIdpoint, out classIdpoint);
                          // sigmoidå¾—åˆ°åˆ†ç±»çš„æ¦‚ç‡
                          max_class_score = Sigmoid((float)max_class_score);
                          if(max_class_score > boxThreshold)
                          {

                              // æ˜¯å¥½çš„é”šæ¡†ï¼Œä¿å­˜,é”šæ¡†å›å½’
                              double x = (Sigmoid(pdata[0]) * 2 - 0.5 + i) * netStride[s];
                              double y = (Sigmoid(pdata[1]) * 2 - 0.5 + j) * netStride[s];
                              double w = Math.Pow(Sigmoid(pdata[2]) * 2.0, 2.0) * anchor_w;
                              double h = Math.Pow(Sigmoid(pdata[3]) * 2.0, 2.0) * anchor_h;
                              //è¿‡æ»¤é¢ç§¯å¤ªå°çš„
                              //if (w >=45 && h >=45)
                              //{
                              //    int left = (int)((x - 0.5 * w) * ratio_w);
                              //    int top = (int)((y - 0.5 * h) * ratio_h);
                              //    classIds.Add(classIdpoint.X);
                              //    confidences.Add((float)max_class_score);
                              //    bouduary.Add(new Rect(left, top, (int)(w * ratio_w), (int)(h * ratio_h)));
                              //}
                          }
                      }
                      ++r;
                  }
              }
          }
      }
    ```  
  - Now,ä½¿ç”¨NMSå¯¹å¤‡é€‰æ¡†è¿›è¡Œç­›é€‰:
   ```c#
    // ä¿å­˜æœ€ç»ˆçš„æœ€å¥½çš„é”šæ¡†åœ¨å¤‡é€‰æ¡†ä¸­çš„index
    int[] final_idx;
    // nmsï¼Œéæå¤§å€¼æŠ‘åˆ¶,æœ‰å¾ˆå¤šé¢„æµ‹é”šæ¡†æ˜¯é‡å çš„ï¼Œæ•…ç”¨nmså¯å¾—åˆ°å…¶ä¸­æœ€å¥½çš„ä¸€ä¸ªé”šæ¡†
    CvDnn.NMSBoxes(bouduary, confidences, classThreshold, nmsThreshold, out final_idx);
    // æ ¹æ®æœ€ç»ˆé”šæ¡†çš„idxï¼Œä¿å­˜ç»“æœ
    for(int i = 0; i < final_idx.Length; i++)
    {
        int idx = final_idx[i];
        output temp;
        temp.id = classIds[idx];   // ä¿®æ”¹åœ°æ–¹ åº”è¯¥è·å–ç±»åˆ«è€Œä¸æ˜¯index
        temp.confidence = confidences[idx]; 
        temp.box = bouduary[idx]; 
        Results.Add(temp);
    }
   ```  
 - ç”¨opencvåœ¨åŸå›¾ä¸Šç”»å‡ºé”šæ¡†ï¼š
 ```c#
     for(int i = 0; i < res.Count; i++)
    {
        int left=res[i].box.X,top = res[i].box.Y;
        Cv2.Rectangle(img, res[i].box, new Scalar(0, 255, 0), thickness: 1, LineTypes.Link8);
        Cv2.PutText(img, (i+1).ToString(), new OpenCvSharp.Point(left, top), HersheyFonts.HersheySimplex, fontScale: 0.35, color: new Scalar(0, 0, 255), thickness: 1);
        if (isDrawLedCenter==true)
        {
            // ç»˜åˆ¶åå­—æ ‡
            int x = res[i].box.X + res[i].box.Width / 2, y = res[i].box.Y + res[i].box.Height / 2;
            OpenCvSharp.Point py1 = new OpenCvSharp.Point(x,y-5);
            OpenCvSharp.Point py2 = new OpenCvSharp.Point(x, y + 5);
            OpenCvSharp.Point px1 = new OpenCvSharp.Point(x-5, y);
            OpenCvSharp.Point px2 = new OpenCvSharp.Point(x+5, y);
            Cv2.Line(img, py1, py2, new Scalar(255,0 , 0)); Cv2.Line(img, px1, px2, new Scalar(255, 0, 0));
        }
    }
 ```  
 
 - è¾“å‡ºOutputç»“æ„ï¼š
 ```c#
    /// <summary>
    /// Outputæ•°æ®ç»“æ„ï¼Œå³ä¸ºé¢„æµ‹é”šæ¡†å‚æ•°
    /// </summary>
    public struct output
    {
        public int id;//ç»“æœç±»åˆ«id
        public float confidence;//ç»“æœç½®ä¿¡åº¦
        public Rect box;//çŸ©å½¢æ¡†
    }
 ```  
 
 - ç»“æœï¼š
 <div align=center>
      <img name="testCsharp" src="https://github.com/lin-tea/YOLOv5DetectionWithCSharp/blob/main/Pictures/testCsharp.png" width="70%" height="65%"></div>   
## Reference:
  [1] [YOLOv5 Document](https://docs.ultralytics.com/).  
  [2] What is Anchor? [Anchor Boxes for Object detection](https://stackoverflow.com/questions/70227234/anchor-boxes-for-object-detection).
  
